{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章: RNN, CNN\n",
    "\n",
    "## 80. ID番号への変換\n",
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.feature.npy  train.feature.npy  valid.feature.npy\n",
      "test.txt\t  train.txt\t     valid.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ../chapter06/work/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greek 10-year yields rise day after five-year sale\tb\n",
      "Tori Spelling - Tori Spelling in 'crisis mode'?\te\n",
      "German Stocks Rise After Two-Week Rally as Sky Deutschland Gains\tb\n",
      "Sinéad O'Connor Gets New Look For 'I'm Not Bossy, I'm The Boss'\te\n",
      "Tom Cruise - Tom Cruise Surprises Fans At Movie Theatre\te\n",
      "BLOGS OF THE DAY: Brad Pitt returns to World War II\te\n",
      "Microsoft CEO Said to Unveil Office for IPad on March 27\tt\n",
      "Even Slightly High Blood Pressure Could Raise Stroke Risk\tm\n",
      "US Yield Over Japan Double Year-Ago Level as BOJ Holds Policy\tb\n",
      "Keri Russell - Keri Russell: Andy Serkis 'unbelievable'\te\n"
     ]
    }
   ],
   "source": [
    "!head ../chapter06/work/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/y_kishinami/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from logzero import logger\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 語彙の作成（学習データに出現する単語に対してID番号を付与）を行う関数\n",
    "def make_vocab(train_path, vocab_path, min_thr):\n",
    "    with open(train_path) as file, open(vocab_path, 'w') as vocab_file:\n",
    "        c = Counter()\n",
    "        vocab = defaultdict(int)\n",
    "\n",
    "        for line in file:\n",
    "            title, category = line.strip().split('\\t')\n",
    "            for word in nltk.word_tokenize(title):\n",
    "                c[word] += 1\n",
    "\n",
    "        for i, (key, f) in enumerate(sorted(c.items(), key=lambda x: x[1], reverse=True), start=1):\n",
    "            if f >= min_thr:\n",
    "                print(key, i, sep='\\t', file=vocab_file)\n",
    "                \n",
    "# 語彙の読み込みを行う関数\n",
    "def load_vocab(vocab_path):\n",
    "    word2id = defaultdict(int)\n",
    "    with open(vocab_path) as vocab_file:\n",
    "        for line in vocab_file:\n",
    "            word, ids = line.strip().split('\\t')\n",
    "            word2id[word] = int(ids)\n",
    "    return word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/Users/y_kishinami/Documents/100knock-2020/y-kishinami/chapter06/work/train.txt'\n",
    "vocab_path = '/Users/y_kishinami/Documents/100knock-2020/y-kishinami/chapter09/work/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 語彙の作成（初回のみ実行）\n",
    "make_vocab(train_path, vocab_path, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\t1\n",
      "...\t2\n",
      ",\t3\n",
      "'s\t4\n",
      "'\t5\n",
      "in\t6\n",
      ":\t7\n",
      "on\t8\n",
      "as\t9\n",
      "-\t10\n"
     ]
    }
   ],
   "source": [
    "!head work/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 9817\n"
     ]
    }
   ],
   "source": [
    "# 語彙のロード（単語→idへの変換辞書）\n",
    "word2id = load_vocab(vocab_path)\n",
    "print('vocab size: {}'.format(len(word2id) + 1))  # 未知語のid:0を含める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tracy', 'Morgan', '-', 'Tracy', 'Morgan', 'is', \"'doing\", 'better', \"'\", 'after', 'crash']\n",
      "tensor([1557,  379,   10, 1557,  379,   47, 8988, 1906,    5,   30, 1446])\n"
     ]
    }
   ],
   "source": [
    "# 単語のリストからid番号列に変換する関数\n",
    "def words2id(words, word2id):\n",
    "    return torch.tensor([word2id[word] if word in word2id.keys() else 0 for word in words])\n",
    "\n",
    "sentence = \"Tracy Morgan - Tracy Morgan is 'doing better' after crash\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)\n",
    "print(words2id(tokens, word2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81. RNNによる予測\n",
    "ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列xからカテゴリyを予測するモデルとして，次式を実装せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1文の単語列->単語数*語彙数のone-hotベクトル\n",
    "def title2id(title, vocab):\n",
    "    ids = words2id(nltk.word_tokenize(title), vocab)\n",
    "    x = torch.zeros(len(ids),len(vocab)+1)  # 0のために１たす\n",
    "    for i, word_id in enumerate(ids):\n",
    "        x[i][word_id] = 1\n",
    "    return x\n",
    "\n",
    "# category→カテゴリID\n",
    "def cat2id(y):\n",
    "    cate = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return torch.tensor(cate[y], dtype=torch.int64)\n",
    "\n",
    "# titleのID列とカテゴリのIDを生成するgenerator\n",
    "def gen_sample(path):\n",
    "    with open(path) as fi:\n",
    "        for line in fi:\n",
    "            title, category = line.strip().split('\\t')\n",
    "            yield words2id(nltk.word_tokenize(title), word2id), cat2id(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNクラス\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, padding_idx, output_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)  # paddingに0を使うと未知語と同じ扱いになっちゃうので、語彙数＋1のIDをpadding_idx\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh', batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size, bias=True)\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        self.batch_size = x.size()[0]\n",
    "        hidden = torch.zeros(1, self.batch_size, self.hidden_size)\n",
    "        emb = self.emb(x)  # emb.size() = (batch_size, seq_len, emb_size)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, x_len, batch_first=True)  # packed.size() = (packed_seq, batch_size)\n",
    "        out, h_n = self.rnn(packed, hidden)  # out.size() = (batch_size, seq_len, hidden_size)\n",
    "        logit = self.fc(h_n[-1])  # out.size() = (batch_size, output_size)\n",
    "        #logit = self.fc(torch.cat([h_n[-2], h_n[-1]], dim=1))\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  3\n",
      "(batch_size, output_size) :  torch.Size([3, 4])\n",
      "sequence length: [8]\n",
      "tensor([[0.2681, 0.1576, 0.1917, 0.3826]], grad_fn=<SoftmaxBackward>)\n",
      "sequence length: [10]\n",
      "tensor([[0.3945, 0.3003, 0.1291, 0.1760]], grad_fn=<SoftmaxBackward>)\n",
      "sequence length: [10]\n",
      "tensor([[0.2202, 0.1835, 0.3774, 0.2189]], grad_fn=<SoftmaxBackward>)\n",
      "sequence length: [17]\n",
      "tensor([[0.3192, 0.2363, 0.2576, 0.1870]], grad_fn=<SoftmaxBackward>)\n",
      "sequence length: [10]\n",
      "tensor([[0.2219, 0.3218, 0.2654, 0.1909]], grad_fn=<SoftmaxBackward>)\n",
      "sequence length: [12]\n",
      "tensor([[0.1577, 0.2620, 0.4155, 0.1648]], grad_fn=<SoftmaxBackward>)\n",
      "sequence length: [11]\n",
      "tensor([[0.2623, 0.2180, 0.2142, 0.3055]], grad_fn=<SoftmaxBackward>)\n",
      "sequence length: [9]\n",
      "tensor([[0.2295, 0.2533, 0.4168, 0.1005]], grad_fn=<SoftmaxBackward>)\n",
      "sequence length: [11]\n",
      "tensor([[0.2379, 0.2031, 0.4576, 0.1014]], grad_fn=<SoftmaxBackward>)\n",
      "sequence length: [10]\n",
      "tensor([[0.2685, 0.2758, 0.2877, 0.1681]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2id) + 1\n",
    "emb_size = 300\n",
    "padding_idx = len(word2id)  # 語彙数idxをpadding idxとして追加\n",
    "output_size = 4  # カテゴリ数\n",
    "hidden_size = 50\n",
    "\n",
    "rnn = RNN(vocab_size, emb_size, padding_idx, output_size, hidden_size)\n",
    "inputs = torch.tensor([[1,4,6,3,5], [1,2,3,4,0], [1,1,0,0,0]])  # 例として適当なID列を入力として与える\n",
    "\n",
    "print('batch size: ', inputs.size()[0])  # batch_size\n",
    "print('(batch_size, output_size) : ', rnn(inputs, [5, 4, 3]).size())  # [batch_size, output_size]\n",
    "\n",
    "# 訓練データの10事例について結果をみてみる\n",
    "for x,y in islice(gen_sample(train_path), 10):\n",
    "    x_lens = [len(x)]\n",
    "    print('sequence length:', x_lens)\n",
    "    print(F.softmax(rnn(x.unsqueeze(0), x_lens), dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 82. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストデータ読み込み関数\n",
    "def load_file(path):\n",
    "    with open(path) as fi:\n",
    "        x, y = [], []\n",
    "        for line in fi:\n",
    "            title, category = line.strip().split('\\t')\n",
    "            tokens = nltk.word_tokenize(title)\n",
    "            x.append(tokens)\n",
    "            y.append(category)\n",
    "        return x, y\n",
    "    \n",
    "    \n",
    "# データセット作成クラス\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, x, y, vocab):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):  # CreateDataset()[index]で返ってくる値を定義\n",
    "        return words2id(self.x[index], self.vocab), cat2id(self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([2243, 3414,  388,  112,  381,   30, 4102, 1166]), tensor(0))\n",
      "train data: 10684 samples\n"
     ]
    }
   ],
   "source": [
    "# テキストデータをtitle, categoryそれぞれのリストとして読み込み\n",
    "train_path = '/Users/y_kishinami/Documents/100knock-2020/y-kishinami/chapter06/work/train.txt'\n",
    "valid_path = '/Users/y_kishinami/Documents/100knock-2020/y-kishinami/chapter06/work/valid.txt'\n",
    "\n",
    "x_train, y_train = load_file(train_path)\n",
    "x_valid, y_valid = load_file(valid_path)\n",
    "\n",
    "# CreateDatasetクラスのインスタンスを生成\n",
    "train_dataset = CreateDataset(x_train, y_train, word2id)\n",
    "valid_dataset = CreateDataset(x_valid, y_valid, word2id)\n",
    "print(train_dataset[0])\n",
    "print('train data: {} samples'.format(len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ロスと正解率の計算を行う関数\n",
    "def calc_loss_and_acc(model, dataset, criterion):\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)  # シャッフルしない\n",
    "    total_loss, total, correct = 0., 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs, [inputs[0].shape[0]])\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "            \n",
    "            pred = torch.argmax(outputs, dim=-1)\n",
    "            total += len(inputs)\n",
    "            correct += (pred == labels).sum().item()\n",
    "        \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "\n",
    "# train\n",
    "def train(model, train_dataset, valid_dataset, batch_size, criterion, optimizer, epoch, collate_fn=None):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "    logger.info('dataset loaded. ')\n",
    "    logger.info('train data: {} samples'.format(len(train_dataset)))\n",
    "    logger.info('valid data: {} samples'.format(len(valid_dataset)))\n",
    "    logger.info('training start.')\n",
    "    \n",
    "    for epoch in range(epoch):\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()        \n",
    "            outputs = model(inputs, [inputs[0].shape[0]])\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss, train_acc = calc_loss_and_acc(model, train_dataset, criterion)\n",
    "        valid_loss, valid_acc = calc_loss_and_acc(model, valid_dataset, criterion)\n",
    "        \n",
    "        print('epoch: {} done. '.format(epoch + 1))\n",
    "        print('train loss: {}\\ttrain acc: {}'.format(train_loss, train_acc))\n",
    "        print('valid loss: {}\\tvalid acc: {}'.format(valid_loss, valid_acc))\n",
    "    logger.info('training done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210118 16:02:00 <ipython-input-192-a966ca17b27e>:21] dataset loaded. \n",
      "[I 210118 16:02:00 <ipython-input-192-a966ca17b27e>:22] train data: 10684 samples\n",
      "[I 210118 16:02:00 <ipython-input-192-a966ca17b27e>:23] valid data: 1336 samples\n",
      "[I 210118 16:02:00 <ipython-input-192-a966ca17b27e>:24] training start.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 done. \n",
      "train loss: 0.8085591446159206\ttrain acc: 0.7227630101085736\n",
      "valid loss: 0.8557747856875558\tvalid acc: 0.7013473053892215\n",
      "epoch: 2 done. \n",
      "train loss: 0.7315529478012676\ttrain acc: 0.7323099962560838\n",
      "valid loss: 0.8814415743882618\tvalid acc: 0.6833832335329342\n",
      "epoch: 3 done. \n",
      "train loss: 0.5407061495804076\ttrain acc: 0.7982029202545863\n",
      "valid loss: 0.7041170801409704\tvalid acc: 0.749251497005988\n",
      "epoch: 4 done. \n",
      "train loss: 0.5042316751375562\ttrain acc: 0.8137401722201423\n",
      "valid loss: 0.7490548531662166\tvalid acc: 0.7327844311377245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210118 16:10:48 <ipython-input-192-a966ca17b27e>:39] training done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 done. \n",
      "train loss: 0.4308029864901095\ttrain acc: 0.8400411830774991\n",
      "valid loss: 0.7236936965320834\tvalid acc: 0.7432634730538922\n"
     ]
    }
   ],
   "source": [
    "# 各種パラメータを設定\n",
    "vocab_size = len(word2id) + 1\n",
    "emb_size = 300\n",
    "padding_idx = vocab_size - 1\n",
    "output_size = 4\n",
    "hidden_size = 50\n",
    "lr = 0.005\n",
    "epoch = 5\n",
    "batch_size = 1\n",
    "\n",
    "# モデル初期化\n",
    "rnn = RNN(vocab_size, emb_size, padding_idx, output_size, hidden_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=lr)\n",
    "\n",
    "# train\n",
    "train(rnn, train_dataset, valid_dataset, batch_size, criterion, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83. ミニバッチ化・GPU上での学習\n",
    "問題82のコードを改変し，B事例ごとに損失・勾配を計算して学習を行えるようにせよ（Bの値は適当に選べ）．また，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPUの学習に使用したコードはsrcディレクトリにあります。（q83.py）\n",
    "- batch_size=8, epoch=30, lr=0.005で実行。validでのaccが最大のチェックポイントのみ保存\n",
    "    - 時間 7分33秒\n",
    "    - train loss: 0.34684245115420537\ttrain acc: 0.8740172220142269\n",
    "    - valid loss: 0.6549646953585443\tvalid acc: 0.7776946107784432\n",
    "    - save checkpoint epoch : 28 acc : 0.7776946107784432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチ化するときには系列長に気をつける必要\n",
    "# 入力データの長さが異なるため、バッチ内で系列長を揃えるためにpadding\n",
    "# collate_fnを自作することで実装（バッチ作成時の挙動を制御）\n",
    "\n",
    "class Padsequence():\n",
    "    \"\"\"Dataloaderからミニバッチを取り出すごとに最大系列長でパディング\"\"\"\n",
    "    def __init__(self, padding_idx):\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)  # batch内の系列が降順になっているとpaddingの効率がいいらしい\n",
    "        sequences = [x[0] for x in sorted_batch]\n",
    "        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=self.padding_idx)  # padding\n",
    "        labels = torch.LongTensor([x[1] for x in sorted_batch])\n",
    "        lens = torch.LongTensor([x[0].shape[0] for x in sorted_batch])\n",
    "        return sequences_padded, labels, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2243, 3414,  388,  112,  381,   30, 4102, 1166]]) tensor([0])\n",
      "tensor([[ 960, 1092,   10,  960, 1092,    6,    0, 6637,    5,   34]]) tensor([2])\n",
      "tensor([[ 327,   97,  174,   41, 5105,  645,    9, 4103, 6638,  359]]) tensor([0])\n",
      "tensor([[   0, 6639,  428,   33,  610,   31,    5,   88,  681,  103, 2012,    3,\n",
      "           88,  681,   14, 4104,    5]]) tensor([2])\n",
      "tensor([[1167, 1808,   10, 1167, 1808, 5106,  403,   49,  137, 3415]]) tensor([2])\n"
     ]
    }
   ],
   "source": [
    "# もともとのdataloaderでどんな感じのデータが得られているのか？\n",
    "for x,y in islice(DataLoader(train_dataset, batch_size=1, collate_fn=None), 5):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence len : 8\n",
      "sequence len : 10\n",
      "sequence len : 10\n",
      "sequence len : 17\n",
      "sequence len : 10\n",
      "sequence len : 12\n",
      "sequence len : 11\n",
      "sequence len : 9\n",
      "sequence len : 11\n",
      "sequence len : 10\n"
     ]
    }
   ],
   "source": [
    "# バッチ内の各サンプルの系列長の取得方法\n",
    "for x in islice(loader = DataLoader(train_dataset, batch_size=1, collate_fn=None), 10):\n",
    "    print('sequence length : {}'.format(x[0][0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 6639,  428,   33,  610,   31,    5,   88,  681,  103, 2012,    3,\n",
      "           88,  681,   14, 4104,    5],\n",
      "        [ 960, 1092,   10,  960, 1092,    6,    0, 6637,    5,   34, 9999, 9999,\n",
      "         9999, 9999, 9999, 9999, 9999],\n",
      "        [ 327,   97,  174,   41, 5105,  645,    9, 4103, 6638,  359, 9999, 9999,\n",
      "         9999, 9999, 9999, 9999, 9999],\n",
      "        [2243, 3414,  388,  112,  381,   30, 4102, 1166, 9999, 9999, 9999, 9999,\n",
      "         9999, 9999, 9999, 9999, 9999]])\n"
     ]
    }
   ],
   "source": [
    "# paddingした結果\n",
    "for d in islice(DataLoader(train_dataset, batch_size=4, collate_fn=Padsequence(9999)), 1):\n",
    "    print(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 6639,  428,   33,  610,   31,    5,   88,  681,  103, 2012,    3,\n",
      "           88,  681,   14, 4104,    5],\n",
      "        [ 960, 1092,   10,  960, 1092,    6,    0, 6637,    5,   34, 9999, 9999,\n",
      "         9999, 9999, 9999, 9999, 9999],\n",
      "        [ 327,   97,  174,   41, 5105,  645,    9, 4103, 6638,  359, 9999, 9999,\n",
      "         9999, 9999, 9999, 9999, 9999],\n",
      "        [2243, 3414,  388,  112,  381,   30, 4102, 1166, 9999, 9999, 9999, 9999,\n",
      "         9999, 9999, 9999, 9999, 9999]])\n",
      "tensor([2, 2, 0, 0])\n",
      "tensor([17, 10, 10,  8])\n"
     ]
    }
   ],
   "source": [
    "# paddingした結果\n",
    "for d in islice(DataLoader(train_dataset, batch_size=4, collate_fn=Padsequence(9999)), 1):\n",
    "    print(d[0])  # バッチの単語ID列\n",
    "    print(d[1])  # バッチの正解ラベル\n",
    "    print(d[2])  # バッチの元の単語数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[   0,  960,  327, 2243],\n",
      "        [6639, 1092,   97, 3414],\n",
      "        [ 428,   10,  174,  388],\n",
      "        [  33,  960,   41,  112],\n",
      "        [ 610, 1092, 5105,  381],\n",
      "        [  31,    6,  645,   30],\n",
      "        [   5,    0,    9, 4102],\n",
      "        [  88, 6637, 4103, 1166],\n",
      "        [ 681,    5, 6638, 9999],\n",
      "        [ 103,   34,  359, 9999],\n",
      "        [2012, 9999, 9999, 9999],\n",
      "        [   3, 9999, 9999, 9999],\n",
      "        [  88, 9999, 9999, 9999],\n",
      "        [ 681, 9999, 9999, 9999],\n",
      "        [  14, 9999, 9999, 9999],\n",
      "        [4104, 9999, 9999, 9999],\n",
      "        [   5, 9999, 9999, 9999]]), tensor([2, 2, 0, 0]), tensor([17, 10, 10,  8]))\n"
     ]
    }
   ],
   "source": [
    "# これはbatch_first=Falseにした結果\n",
    "# このようになっちゃう。batch_first=Trueにすることで1次元目にバッチサイズを持ってこれる\n",
    "for d in islice(DataLoader(train_dataset, batch_size=4, collate_fn=Padsequence(9999)), 1):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 84. 単語ベクトルの導入\n",
    "事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル）で単語埋め込みemb(x)を初期化し，学習せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPUの学習に使用したコードはsrcディレクトリにあります。（q84.py）\n",
    "- batch_size=8, epoch=30, lr=0.005で実行。validでのaccが最大のチェックポイントのみ保存\n",
    "    - train loss: 0.2578477888207111\ttrain acc: 0.9141707225758143\n",
    "    - valid loss: 0.3482527509035437\tvalid acc: 0.8802395209580839\n",
    "    - save checkpoint epoch : 29 acc : 0.8802395209580839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print(len(model.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9817, 300)\n"
     ]
    }
   ],
   "source": [
    "pretrained_emb_w = np.zeros((vocab_size, 300))\n",
    "# vocabのIDに対応する単語を取得\n",
    "for k,v in word2id.items():\n",
    "    try:\n",
    "        if v == 0:\n",
    "            continue\n",
    "        # 取得した単語の300次元のベクトルを取得\n",
    "        # 取得したベクトルを重みベクトルの行に追加\n",
    "        pretrained_emb[v] = model[k]\n",
    "    except KeyError:\n",
    "        continue\n",
    "print(pretrained_emb_w.shape)\n",
    "\n",
    "# vocab * emb_size(300)のnumpy行列ができる\n",
    "# この行列をembedding層の重みにセットする\n",
    "emb = nn.Embedding.from_pretrained(torch.from_numpy(pretrained_emb_w), padding_idx=9816)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/q84.py\n"
     ]
    }
   ],
   "source": [
    "%%file 'src/q84.py'\n",
    "import argparse\n",
    "from os import path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "from logzero import logger\n",
    "nltk.download('punkt')\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, x, y, vocab):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):  # CreateDataset()[index]で返ってくる値を定義\n",
    "        return words2id(self.x[index], self.vocab), cat2id(self.y[index])\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, padding_idx, output_size, hidden_size, vec_path, word2id):\n",
    "        super().__init__()\n",
    "        self.to(device)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = nn.Embedding.from_pretrained(load_pretrained_vector(vec_path, word2id, vocab_size), padding_idx=padding_idx)\n",
    "        #self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)  # paddingに0を使うと未知語と同じ扱いになっちゃうので、語彙数-1のIDをpadding_idxにする\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh', batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size, bias=True)\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        self.batch_size = x.size()[0]\n",
    "        hidden = torch.zeros(1, self.batch_size, self.hidden_size)\n",
    "        emb = self.emb(x)\n",
    "        # emb.size() = (batch_size, seq_len, emb_size)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, x_len, batch_first=True, enforce_sorted=False)  # packing済み, 元の系列長→packed_sequence\n",
    "        out, h_n = self.rnn(packed, hidden)\n",
    "        # out.size() = (batch_size, seq_len, hidden_size)\n",
    "        logit = self.fc(h_n[-1])\n",
    "        # out.size() = (batch_size, output_size)\n",
    "        return logit\n",
    "\n",
    "\n",
    "class Padsequence():\n",
    "    \"\"\"Dataloaderからミニバッチを取り出すごとに最大系列長でパディング\"\"\"\n",
    "    def __init__(self, padding_idx):\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)  # batch内の系列が降順になっているとpaddingの効率がいいらしい\n",
    "        sequences = [x[0] for x in sorted_batch]\n",
    "        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=self.padding_idx)  # padding\n",
    "        labels = torch.LongTensor([x[1] for x in sorted_batch])\n",
    "        lens = torch.LongTensor([x[0].shape[0] for x in sorted_batch])\n",
    "        return sequences_padded, labels, lens\n",
    "\n",
    "\n",
    "# text fileの読み込みを行う関数\n",
    "def load_file(path):\n",
    "    with open(path) as fi:\n",
    "        x, y = [],[]\n",
    "        for line in fi:\n",
    "            title, category = line.strip().split('\\t')\n",
    "            x.append(nltk.word_tokenize(title))\n",
    "            y.append(category)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# 単語のリストからid番号列に変換する関数\n",
    "def words2id(words, word2id):\n",
    "    return torch.tensor([word2id[word] if word in word2id.keys() else 0 for word in words])\n",
    "\n",
    "\n",
    "# カテゴリ名からカテゴリidに変換する関数\n",
    "def cat2id(y):\n",
    "    cate = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return torch.tensor(cate[y], dtype=torch.int64)\n",
    "\n",
    "\n",
    "# vocab fileの読み込みを行う関数\n",
    "def load_vocab(vocab_path):\n",
    "    word2id = defaultdict(int)\n",
    "    with open(vocab_path) as vocab_file:\n",
    "        for line in vocab_file:\n",
    "            word, ids = line.strip().split('\\t')\n",
    "            word2id[word] = int(ids)\n",
    "    return word2id\n",
    "\n",
    "\n",
    "def load_pretrained_vector(vec_path, word2id, vocab_size):\n",
    "    pretrained_vec_w = np.zeros((vocab_size, 300))\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(vec_path, binary=True)\n",
    "    logger.info('pretrained word2vec vocab size: {}'.format(len(model.vocab)))\n",
    "    for key, value in word2id.items():\n",
    "        try:\n",
    "            if value == 0:continue  # 未知語はzeroのまま\n",
    "            pretrained_vec_w[value] = model[key]\n",
    "        except KeyError:  # 学習済みword2vecに含まれていない単語もzeroのまま\n",
    "            continue\n",
    "    return torch.from_numpy(pretrained_vec_w.astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "# loss, accを計算する関数\n",
    "def calc_loss_and_acc(model, dataset, criterion):\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    total_loss, total, correct = 0., 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs, [inputs[0].shape[0]])\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "            pred = torch.argmax(outputs, dim=-1)\n",
    "            total += len(inputs)\n",
    "            correct += (pred == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "\n",
    "# 学習\n",
    "def train(model, train_dataset, valid_dataset, batch_size, criterion, optimizer, epoch, device, model_path, collate_fn=None):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    best_acc = 0.\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        for data in train_dataloader:\n",
    "            inputs, labels, inputs_len = data\n",
    "            optimizer.zero_grad()\n",
    "            inputs.to(device)\n",
    "            labels.to(device)\n",
    "            inputs_len.to(device)\n",
    "            outputs = model(inputs, inputs_len)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss, train_acc = calc_loss_and_acc(model, train_dataset, criterion)\n",
    "        valid_loss, valid_acc = calc_loss_and_acc(model, valid_dataset, criterion)\n",
    "\n",
    "        print('epoch: {} done. '.format(epoch + 1))\n",
    "        print('train loss: {}\\ttrain acc: {}'.format(train_loss, train_acc))\n",
    "        print('valid loss: {}\\tvalid acc: {}'.format(valid_loss, valid_acc))\n",
    "\n",
    "        # validのaccの最大値が更新されたらそのチェックポイントを保存\n",
    "        if best_acc <= valid_acc:\n",
    "            best_acc = valid_acc\n",
    "            torch.save({\n",
    "                'epoch':epoch+1,\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict()\n",
    "            }, model_path)\n",
    "            print(\"save checkpoint epoch : {} acc : {}\".format(epoch+1, valid_acc))\n",
    "\n",
    "\n",
    "\n",
    "# argument\n",
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser(description='hogehoge')\n",
    "    parser.add_argument('--vocab_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/vocab.txt', type=path.abspath, help='Path to vocabulary file')\n",
    "    parser.add_argument('--train_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/train.txt', type=path.abspath, help='Path to train data file')\n",
    "    parser.add_argument('--valid_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/valid.txt', type=path.abspath, help='Path to valid data file')\n",
    "    parser.add_argument('--model_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/q84_checkpoint_best.pt', type=path.abspath, help='Path to save checkpoint best model')\n",
    "    parser.add_argument('--vec_path', default='/work01/y_kishinami/100knock-2020/chapter09/GoogleNews-vectors-negative300.bin', type=path.abspath, help='Path to pretrained word2vec')\n",
    "    parser.add_argument('--emb_size', default=300, type=int, help='dimension of embedding layer')\n",
    "    parser.add_argument('--output_size', default=4, type=int, help='dimension of output layer')\n",
    "    parser.add_argument('--hidden_size', default=50, type=int, help='dimension of hidden layer')\n",
    "    parser.add_argument('--batch_size', default=1, type=int, help='batch size')\n",
    "    parser.add_argument('--lr', default=0.005, type=float, help='learning late')\n",
    "    parser.add_argument('--epoch', default=10, type=int, help='the number of epoch')\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main():\n",
    "    # argument\n",
    "    parser = create_parser()\n",
    "    args = parser.parse_args()\n",
    "    logger.info(args)\n",
    "\n",
    "    # 語彙のload（単語→idへの変換辞書）\n",
    "    word2id = load_vocab(args.vocab_path)\n",
    "    vocab_size = len(word2id) + 1\n",
    "    logger.info('vocabulary loaded. vocab size: {}'.format(vocab_size))\n",
    "\n",
    "    # datasetのload\n",
    "    logger.info('dataset loading ...')\n",
    "    x_train, y_train = load_file(args.train_path)\n",
    "    x_valid, y_valid = load_file(args.valid_path)\n",
    "    train_dataset = CreateDataset(x_train, y_train, word2id)\n",
    "    valid_dataset = CreateDataset(x_valid, y_valid, word2id)\n",
    "    logger.info('dataset loaded. ')\n",
    "    logger.info('train data: {} samples'.format(len(train_dataset)))\n",
    "    logger.info('valid data: {} samples'.format(len(valid_dataset)))\n",
    "\n",
    "    # モデルの初期化\n",
    "    rnn = RNN(vocab_size, args.emb_size, vocab_size-1, args.output_size, args.hidden_size, args.vec_path, word2id)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(rnn.parameters(), lr=args.lr)\n",
    "\n",
    "\n",
    "    # 学習\n",
    "    logger.info('training start !')\n",
    "    train(rnn, train_dataset, valid_dataset, args.batch_size, criterion, optimizer, args.epoch, device, args.model_path, collate_fn=Padsequence(vocab_size-1))\n",
    "    logger.info('training done !')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85. 双方向RNN・多層化\n",
    "順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- コードはsrcにおきました（q85.py）\n",
    "- batch_size=64, epoch=5, lr=0.01, dropout=0.6, adagradで実行\n",
    "    - train loss: 0.27643798515469065\ttrain acc: 0.9082740546611756\n",
    "    - valid loss: 0.33827637885619266\tvalid acc: 0.8877245508982036\n",
    "    - save checkpoint epoch : 3 acc : 0.8877245508982036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/q85.py\n"
     ]
    }
   ],
   "source": [
    "%%file 'src/q85.py'\n",
    "import argparse\n",
    "from os import path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "from logzero import logger\n",
    "nltk.download('punkt')\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, x, y, vocab):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):  # CreateDataset()[index]で返ってくる値を定義\n",
    "        return words2id(self.x[index], self.vocab), cat2id(self.y[index])\n",
    "\n",
    "\n",
    "class biRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, padding_idx, output_size, hidden_size, vec_path, word2id, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.to(device)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.emb = nn.Embedding.from_pretrained(load_pretrained_vector(vec_path, word2id, vocab_size), padding_idx=padding_idx)  # paddingに0を使うと未知語と同じ扱いになっちゃうので、語彙数-1のIDをpadding_idxにする\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='relu', batch_first=True, bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(2*hidden_size, output_size, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        self.batch_size = x.size()[0]\n",
    "        hidden = torch.zeros(2*self.num_layers, self.batch_size, self.hidden_size)\n",
    "        emb = self.emb(x)\n",
    "        # emb.size() = (batch_size, seq_len, emb_size)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, x_len, batch_first=True, enforce_sorted=False)  # packing済み, 元の系列長→packed_sequence\n",
    "        out, h_n = self.rnn(packed, hidden)\n",
    "        # out.size() = (batch_size, seq_len, hidden_size)\n",
    "        # h_n.size() = (順方向か逆方向か, batch_size, hidden_size)\n",
    "        logit = self.fc(torch.cat([h_n[-2], h_n[-1]], dim=1))  # -2が最終層の順方向の隠れ層，-1が最終層の逆方向の隠れ層\n",
    "        # out.size() = (batch_size, output_size)\n",
    "        return logit\n",
    "\n",
    "\n",
    "class Padsequence():\n",
    "    \"\"\"Dataloaderからミニバッチを取り出すごとに最大系列長でパディング\"\"\"\n",
    "    def __init__(self, padding_idx):\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)  # batch内の系列が降順になっているとpaddingの効率がいいらしい\n",
    "        sequences = [x[0] for x in sorted_batch]\n",
    "        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=self.padding_idx)  # padding\n",
    "        labels = torch.LongTensor([x[1] for x in sorted_batch])\n",
    "        lens = torch.LongTensor([x[0].shape[0] for x in sorted_batch])\n",
    "        return sequences_padded, labels, lens\n",
    "\n",
    "\n",
    "# text fileの読み込みを行う関数\n",
    "def load_file(path):\n",
    "    with open(path) as fi:\n",
    "        x, y = [],[]\n",
    "        for line in fi:\n",
    "            title, category = line.strip().split('\\t')\n",
    "            x.append(nltk.word_tokenize(title))\n",
    "            y.append(category)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# 単語のリストからid番号列に変換する関数\n",
    "def words2id(words, word2id):\n",
    "    return torch.tensor([word2id[word] if word in word2id.keys() else 0 for word in words])\n",
    "\n",
    "\n",
    "# カテゴリ名からカテゴリidに変換する関数\n",
    "def cat2id(y):\n",
    "    cate = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return torch.tensor(cate[y], dtype=torch.int64)\n",
    "\n",
    "\n",
    "# vocab fileの読み込みを行う関数\n",
    "def load_vocab(vocab_path):\n",
    "    word2id = defaultdict(int)\n",
    "    with open(vocab_path) as vocab_file:\n",
    "        for line in vocab_file:\n",
    "            word, ids = line.strip().split('\\t')\n",
    "            word2id[word] = int(ids)\n",
    "    return word2id\n",
    "\n",
    "\n",
    "def load_pretrained_vector(vec_path, word2id, vocab_size):\n",
    "    pretrained_vec_w = np.zeros((vocab_size, 300))\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(vec_path, binary=True)\n",
    "    logger.info('pretrained word2vec vocab size: {}'.format(len(model.vocab)))\n",
    "    for key, value in word2id.items():\n",
    "        try:\n",
    "            if value == 0:continue  # 未知語はzeroのまま\n",
    "            pretrained_vec_w[value] = model[key]\n",
    "        except KeyError:  # 学習済みword2vecに含まれていない単語もzeroのまま\n",
    "            continue\n",
    "    return torch.from_numpy(pretrained_vec_w.astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "# loss, accを計算する関数\n",
    "def calc_loss_and_acc(model, dataset, criterion):\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    total_loss, total, correct = 0., 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs, [inputs[0].shape[0]])\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "            pred = torch.argmax(outputs, dim=-1)\n",
    "            total += len(inputs)\n",
    "            correct += (pred == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "\n",
    "# 学習\n",
    "def train(model, train_dataset, valid_dataset, batch_size, criterion, optimizer, epoch, device, model_path, collate_fn=None):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    best_acc = 0.\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        for data in train_dataloader:\n",
    "            inputs, labels, inputs_len = data\n",
    "            optimizer.zero_grad()\n",
    "            inputs.to(device)\n",
    "            labels.to(device)\n",
    "            inputs_len.to(device)\n",
    "            outputs = model(inputs, inputs_len)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss, train_acc = calc_loss_and_acc(model, train_dataset, criterion)\n",
    "        valid_loss, valid_acc = calc_loss_and_acc(model, valid_dataset, criterion)\n",
    "\n",
    "        print('epoch: {} done. '.format(epoch + 1))\n",
    "        print('train loss: {}\\ttrain acc: {}'.format(train_loss, train_acc))\n",
    "        print('valid loss: {}\\tvalid acc: {}'.format(valid_loss, valid_acc))\n",
    "\n",
    "        # validのaccの最大値が更新されたらそのチェックポイントを保存\n",
    "        if best_acc <= valid_acc:\n",
    "            best_acc = valid_acc\n",
    "            torch.save({\n",
    "                'epoch':epoch+1,\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict()\n",
    "            }, model_path)\n",
    "            print(\"save checkpoint epoch : {} acc : {}\".format(epoch+1, valid_acc))\n",
    "\n",
    "\n",
    "\n",
    "# argument\n",
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser(description='hogehoge')\n",
    "    parser.add_argument('--vocab_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/vocab.txt', type=path.abspath, help='Path to vocabulary file')\n",
    "    parser.add_argument('--train_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/train.txt', type=path.abspath, help='Path to train data file')\n",
    "    parser.add_argument('--valid_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/valid.txt', type=path.abspath, help='Path to valid data file')\n",
    "    parser.add_argument('--model_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/q85_checkpoint_best.pt', type=path.abspath, help='Path to save checkpoint best model')\n",
    "    parser.add_argument('--vec_path', default='/work01/y_kishinami/100knock-2020/chapter09/GoogleNews-vectors-negative300.bin', type=path.abspath, help='Path to pretrained word2vec')\n",
    "    parser.add_argument('--emb_size', default=300, type=int, help='dimension of embedding layer')\n",
    "    parser.add_argument('--output_size', default=4, type=int, help='dimension of output layer')\n",
    "    parser.add_argument('--hidden_size', default=50, type=int, help='dimension of hidden layer')\n",
    "    parser.add_argument('--batch_size', default=1, type=int, help='batch size')\n",
    "    parser.add_argument('--lr', default=0.005, type=float, help='learning late')\n",
    "    parser.add_argument('--epoch', default=10, type=int, help='the number of epoch')\n",
    "    parser.add_argument('--num_layers', default=1, type=int, help='the number of RNN layers')\n",
    "    parser.add_argument('--dropout', default=0.6, help='dropout ratio')\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main():\n",
    "    # argument\n",
    "    parser = create_parser()\n",
    "    args = parser.parse_args()\n",
    "    logger.info(args)\n",
    "\n",
    "    # 語彙のload（単語→idへの変換辞書）\n",
    "    word2id = load_vocab(args.vocab_path)\n",
    "    vocab_size = len(word2id) + 1\n",
    "    logger.info('vocabulary loaded. vocab size: {}'.format(vocab_size))\n",
    "\n",
    "    # datasetのload\n",
    "    logger.info('dataset loading ...')\n",
    "    x_train, y_train = load_file(args.train_path)\n",
    "    x_valid, y_valid = load_file(args.valid_path)\n",
    "    train_dataset = CreateDataset(x_train, y_train, word2id)\n",
    "    valid_dataset = CreateDataset(x_valid, y_valid, word2id)\n",
    "    logger.info('dataset loaded. ')\n",
    "    logger.info('train data: {} samples'.format(len(train_dataset)))\n",
    "    logger.info('valid data: {} samples'.format(len(valid_dataset)))\n",
    "\n",
    "    # モデルの初期化\n",
    "    rnn = biRNN(vocab_size, args.emb_size, vocab_size-1, args.output_size, args.hidden_size, args.vec_path, word2id, args.num_layers, args.dropout)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adagrad(rnn.parameters(), lr_decay=0.001, lr=args.lr)\n",
    "\n",
    "    # 学習\n",
    "    logger.info('training start !')\n",
    "    train(rnn, train_dataset, valid_dataset, args.batch_size, criterion, optimizer, args.epoch, device, args.model_path, collate_fn=Padsequence(vocab_size-1))\n",
    "    logger.info('training done !')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. 畳み込みニューラルネットワーク (CNN)\n",
    "ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列xからカテゴリyを予測するモデルを実装せよ．\n",
    "\n",
    "ただし，畳み込みニューラルネットワークの構成は以下の通りとする．\n",
    "\n",
    "- 単語埋め込みの次元数: dw\n",
    "- 畳み込みのフィルターのサイズ: 3 トークン\n",
    "- 畳み込みのストライド: 1 トークン\n",
    "- 畳み込みのパディング: あり\n",
    "- 畳み込み演算後の各時刻のベクトルの次元数: dh\n",
    "- 畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文をdh次元の隠れベクトルで表現\n",
    "\n",
    "すなわち，時刻tの特徴ベクトルpt∈ℝdhは次式で表される．\n",
    "\n",
    "- pt=g(W(px)[emb(xt−1);emb(xt);emb(xt+1)]+b(p))\n",
    "\n",
    "ただし，W(px)∈ℝdh×3dw,b(p)∈ℝdhはCNNのパラメータ，gは活性化関数（例えばtanhやReLUなど），[a;b;c]はベクトルa,b,cの連結である．なお，行列W(px)の列数が3dwになるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．\n",
    "\n",
    "最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトルc∈ℝdhを求める．c[i]でベクトルcのi番目の次元の値を表すことにすると，最大値プーリングは次式で表される．\n",
    "\n",
    "- c[i]=max1≤t≤Tpt[i]\n",
    "\n",
    "最後に，入力文書の特徴ベクトルcに行列W(yc)∈ℝL×dhとバイアス項b(y)∈ℝLによる線形変換とソフトマックス関数を適用し，カテゴリyを予測する．\n",
    "\n",
    "- y=softmax(W(yc)c+b(y))\n",
    "\n",
    "なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列でyを計算するだけでよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, padding_idx, output_size, vocab_size, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size  # 問題文のd_h\n",
    "        self.embedding_size = embedding_size  # 単語埋め込みの次元数\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
    "        self.cnn = nn.Conv1d(embedding_size, hidden_size, kernel_size, stride=1, padding=1)  \n",
    "        # 順にinput_channel, output_channel, カーネルのサイズ（フィルタのサイズ）, 畳み込みのストライド, 畳み込みのpaddingの数（1なら系列の両端に1つずつpaddingが入る）\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        # emb.size() = (batch_size, seq_len, embbeding_size)\n",
    "        # emb.transpose(-1, -2).size() = (batch_size, embedding_size, seq_len)\n",
    "        conv = self.cnn(emb.transpose(-1, -2))  # seq_len方向に畳み込むために，seq_lenを最後の次元に持ってくる\n",
    "        # conv.size() = (batch_size, hidden_size, 時刻数？(paddingの数によって変わるので...))\n",
    "        act = F.relu(conv)\n",
    "        # act.size() = (batch_size, hidden_size, 時刻数？)\n",
    "        max_pool = nn.MaxPool1d(act.size()[-1])(act)\n",
    "        # d_hの次元ごとに，全時刻の最大値をとってくる\n",
    "        # max_pool.size() = (batch_size, hidden_size, 1)\n",
    "        # squeezeはmax_poolの次元(batch_size, hidden_size, 1)の1が不要なので消してる\n",
    "        logit = self.fc(torch.squeeze(max_pool, -1))\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2id) + 1\n",
    "emb_size = 300\n",
    "padding_idx = len(word2id)  # 語彙数idxをpadding idxとして追加\n",
    "output_size = 4  # カテゴリ数\n",
    "hidden_size = 50\n",
    "\n",
    "\n",
    "cnn = CNN(hidden_size, emb_size, padding_idx, output_size, vocab_size)\n",
    "\n",
    "x = torch.tensor([[1557, 379, 10, 1557, 379], [47, 8988, 1906, 5, 30]])  # batch_size = 2, seq_len=5のsample\n",
    "print(cnn(x).size()) # (batch_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 1\tgold: 0\n",
      "prediction: 2\tgold: 2\n",
      "prediction: 2\tgold: 0\n",
      "prediction: 2\tgold: 2\n",
      "prediction: 2\tgold: 2\n",
      "prediction: 2\tgold: 2\n",
      "prediction: 2\tgold: 1\n",
      "prediction: 2\tgold: 3\n",
      "prediction: 2\tgold: 0\n",
      "prediction: 2\tgold: 2\n"
     ]
    }
   ],
   "source": [
    "# 10 sampleで実行\n",
    "for x,y in islice(gen_sample(train_path), 10):\n",
    "    pred = F.softmax(cnn(x.unsqueeze(0)), dim=-1)\n",
    "    print('prediction: {}\\tgold: {}'.format(torch.argmax(pred), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://programmer.group/pytorch-learning-conv1d-conv2d-and-conv3d.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. 確率的勾配降下法によるCNNの学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logはtensorboardで保存するようにした\n",
    "- 50 epoch, SGD, batch size 128\n",
    "    - train acc 0.9342\n",
    "    - valid acc 0.8106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/q87.py\n"
     ]
    }
   ],
   "source": [
    "%%file 'src/q87.py'\n",
    "import argparse\n",
    "from os import path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "from logzero import logger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "nltk.download('punkt')\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, x, y, vocab):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):  # CreateDataset()[index]で返ってくる値を定義\n",
    "        return words2id(self.x[index], self.vocab), cat2id(self.y[index])\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, padding_idx, output_size, vocab_size, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size  # 問題文のd_h\n",
    "        self.embedding_size = embedding_size  # 単語埋め込みの次元数\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
    "        self.cnn = nn.Conv1d(embedding_size, hidden_size, kernel_size, stride=1, padding=1)\n",
    "        # 順にinput_channel, output_channel, カーネルのサイズ（フィルタのサイズ）, 畳み込みのストライド, 畳み込みのpaddingの数（1なら系列の両端に1つずつpaddingが入る）\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        # emb.size() = (batch_size, seq_len, embbeding_size)\n",
    "        # emb.transpose(-1, -2).size() = (batch_size, embedding_size, seq_len)\n",
    "        conv = self.cnn(emb.transpose(-1, -2))  # seq_len方向に畳み込むために，seq_lenを最後の次元に持ってくる\n",
    "        # conv.size() = (batch_size, hidden_size, 時刻数？(paddingの数によって変わるので...))\n",
    "        act = F.relu(conv)\n",
    "        # act.size() = (batch_size, hidden_size, 時刻数？)\n",
    "        max_pool = nn.MaxPool1d(act.size()[-1])(act)\n",
    "        # d_hの次元ごとに，全時刻の最大値をとってくる\n",
    "        # max_pool.size() = (batch_size, hidden_size)\n",
    "        # squeezeはmax_poolの次元(batch_size, hidden_size, 1)の1が不要なので消してる\n",
    "        logit = self.fc(torch.squeeze(max_pool, -1))\n",
    "        return logit\n",
    "\n",
    "\n",
    "class Padsequence():\n",
    "    \"\"\"Dataloaderからミニバッチを取り出すごとに最大系列長でパディング\"\"\"\n",
    "    def __init__(self, padding_idx):\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)  # batch内の系列が降順になっているとpaddingの効率がいいらしい\n",
    "        sequences = [x[0] for x in sorted_batch]\n",
    "        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=self.padding_idx)  # padding\n",
    "        labels = torch.LongTensor([x[1] for x in sorted_batch])\n",
    "        return sequences_padded, labels\n",
    "\n",
    "\n",
    "# text fileの読み込みを行う関数\n",
    "def load_file(path):\n",
    "    with open(path) as fi:\n",
    "        x, y = [],[]\n",
    "        for line in fi:\n",
    "            title, category = line.strip().split('\\t')\n",
    "            x.append(nltk.word_tokenize(title))\n",
    "            y.append(category)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# 単語のリストからid番号列に変換する関数\n",
    "def words2id(words, word2id):\n",
    "    return torch.tensor([word2id[word] if word in word2id.keys() else 0 for word in words])\n",
    "\n",
    "\n",
    "# カテゴリ名からカテゴリidに変換する関数\n",
    "def cat2id(y):\n",
    "    cate = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return torch.tensor(cate[y], dtype=torch.int64)\n",
    "\n",
    "\n",
    "# vocab fileの読み込みを行う関数\n",
    "def load_vocab(vocab_path):\n",
    "    word2id = defaultdict(int)\n",
    "    with open(vocab_path) as vocab_file:\n",
    "        for line in vocab_file:\n",
    "            word, ids = line.strip().split('\\t')\n",
    "            word2id[word] = int(ids)\n",
    "    return word2id\n",
    "\n",
    "\n",
    "# loss, accを計算する関数\n",
    "def calc_loss_and_acc(model, dataset, criterion):\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    total_loss, total, correct = 0., 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "            pred = torch.argmax(outputs, dim=-1)\n",
    "            total += len(inputs)\n",
    "            correct += (pred == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "\n",
    "# 学習\n",
    "def train(model, train_dataset, valid_dataset, batch_size, criterion, optimizer, epoch, device, model_path, writer, collate_fn=None):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    best_acc = 0.\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs.to(device)\n",
    "            labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss, train_acc = calc_loss_and_acc(model, train_dataset, criterion)\n",
    "        valid_loss, valid_acc = calc_loss_and_acc(model, valid_dataset, criterion)\n",
    "\n",
    "        # q87追記：tensorboardでlogを保存\n",
    "        writer.add_scalar('train_loss', train_loss, epoch + 1)\n",
    "        writer.add_scalar('train_acc', train_acc, epoch + 1)\n",
    "        writer.add_scalar('valid_loss', valid_loss, epoch + 1)\n",
    "        writer.add_scalar('valid_acc', valid_acc, epoch + 1)\n",
    "        print('epoch: {} done. '.format(epoch + 1))\n",
    "        print('train loss: {}\\ttrain acc: {}'.format(train_loss, train_acc))\n",
    "        print('valid loss: {}\\tvalid acc: {}'.format(valid_loss, valid_acc))\n",
    "\n",
    "        # validのaccの最大値が更新されたらそのチェックポイントを保存\n",
    "        if best_acc <= valid_acc:\n",
    "            best_acc = valid_acc\n",
    "            torch.save({\n",
    "                'epoch':epoch+1,\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict()\n",
    "            }, model_path)\n",
    "            print(\"save checkpoint epoch : {} acc : {}\".format(epoch+1, valid_acc))\n",
    "\n",
    "# argument\n",
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser(description='hogehoge')\n",
    "    parser.add_argument('--vocab_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/vocab.txt', type=path.abspath, help='Path to vocabulary file')\n",
    "    parser.add_argument('--train_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/train.txt', type=path.abspath, help='Path to train data file')\n",
    "    parser.add_argument('--valid_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/valid.txt', type=path.abspath, help='Path to valid data file')\n",
    "    parser.add_argument('--model_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/checkpoint_best_q87.pt', type=path.abspath, help='path to save checkpoint best model')\n",
    "    parser.add_argument('--log_dir', default='/work01/y_kishinami/100knock-2020/chapter09/work/logs/q87', type=path.abspath, help='tensorboard log dir')\n",
    "    parser.add_argument('--emb_size', default=300, type=int, help='dimension of embedding layer')\n",
    "    parser.add_argument('--output_size', default=4, type=int, help='dimension of output layer')\n",
    "    parser.add_argument('--hidden_size', default=50, type=int, help='dimension of hidden layer')\n",
    "    parser.add_argument('--batch_size', default=1, type=int, help='batch size')\n",
    "    parser.add_argument('--lr', default=0.005, type=float, help='learning late')\n",
    "    parser.add_argument('--epoch', default=10, type=int, help='the number of epoch')\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main():\n",
    "    # argument\n",
    "    parser = create_parser()\n",
    "    args = parser.parse_args()\n",
    "    logger.info(args)\n",
    "    writer = SummaryWriter(log_dir=args.log_dir)\n",
    "\n",
    "    # 語彙のload（単語→idへの変換辞書）\n",
    "    word2id = load_vocab(args.vocab_path)\n",
    "    vocab_size = len(word2id) + 1\n",
    "    logger.info('vocabulary loaded. vocab size: {}'.format(vocab_size))\n",
    "\n",
    "    # datasetのload\n",
    "    logger.info('dataset loading ...')\n",
    "    x_train, y_train = load_file(args.train_path)\n",
    "    x_valid, y_valid = load_file(args.valid_path)\n",
    "    train_dataset = CreateDataset(x_train, y_train, word2id)\n",
    "    valid_dataset = CreateDataset(x_valid, y_valid, word2id)\n",
    "    logger.info('dataset loaded. ')\n",
    "    logger.info('train data: {} samples'.format(len(train_dataset)))\n",
    "    logger.info('valid data: {} samples'.format(len(valid_dataset)))\n",
    "\n",
    "    # モデルの初期化\n",
    "    cnn = CNN(args.hidden_size, args.emb_size, vocab_size-1, args.output_size, vocab_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(cnn.parameters(), lr=args.lr)\n",
    "\n",
    "\n",
    "    # 学習\n",
    "    logger.info('training start !')\n",
    "    train(cnn, train_dataset, valid_dataset, args.batch_size, criterion, optimizer, args.epoch, device, args.model_path, writer, collate_fn=Padsequence(vocab_size-1))\n",
    "    logger.info('training done !')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88. パラメータチューニング\n",
    "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RNNのハイパラ探索設定\n",
    "- 探索パラメータ\n",
    "    - learning rate\n",
    "    - batch size\n",
    "    - num layers\n",
    "    - drop out ratio\n",
    "- 固定パラメータ\n",
    "    - emb size = 300(pretrained word2vec)\n",
    "    - output size = 4\n",
    "    - bidirectional = True\n",
    "    - epoch size = 5\n",
    "    - criterion = CrossEntropyLoss\n",
    "    - optimizer = Adagrad\n",
    "2. RNNのハイパラ探索結果\n",
    "- Optunaを使って1時間探索\n",
    "- 最適パラメータ\n",
    "    - {'learning_rate': 0.018291482272472147, 'batch_size': 32.0, 'num_layers': 1, 'drop_out': 0.7113535005616372, 'hidden_size': 256}\n",
    "    - train loss: 0.12620148336212159\ttrain acc: 0.9593785099213777 (5 epoch)\n",
    "    - valid loss: 0.2780322267644646\tvalid acc: 0.9086826347305389 (5 epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/q88.py\n"
     ]
    }
   ],
   "source": [
    "%%file 'src/q88.py' \n",
    "import optuna\n",
    "from logzero import logger\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, x, y, vocab):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):  # CreateDataset()[index]で返ってくる値を定義\n",
    "        return words2id(self.x[index], self.vocab), cat2id(self.y[index])\n",
    "\n",
    "\n",
    "class biRNN(nn.Module):\n",
    "    def __init__(self, emb_size, padding_idx, output_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.to(device)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.emb = nn.Embedding.from_pretrained(torch.from_numpy(np.load('/work01/y_kishinami/100knock-2020/chapter09/work/pretrained_vec.npy').astype(np.float32)), padding_idx=padding_idx)\n",
    "        # paddingに0を使うと未知語と同じ扱いになっちゃうので、語彙数-1のIDをpadding_idxにする\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='relu', batch_first=True, bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(2*hidden_size, output_size, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        self.batch_size = x.size()[0]\n",
    "        hidden = torch.zeros(2*self.num_layers, self.batch_size, self.hidden_size)\n",
    "        emb = self.emb(x)\n",
    "        # emb.size() = (batch_size, seq_len, emb_size)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, x_len, batch_first=True, enforce_sorted=False)  # packing済み, 元の系列長→packed_sequence\n",
    "        out, h_n = self.rnn(packed, hidden)\n",
    "        # out.size() = (batch_size, seq_len, hidden_size)\n",
    "        # h_n.size() = (順方向か逆方向か, batch_size, hidden_size)\n",
    "        logit = self.fc(torch.cat([h_n[-2], h_n[-1]], dim=1))  # -2が最終層の順方向の隠れ層，-1が最終層の逆方向の隠れ層\n",
    "        # out.size() = (batch_size, output_size)\n",
    "        return logit\n",
    "\n",
    "\n",
    "class Padsequence():\n",
    "    \"\"\"Dataloaderからミニバッチを取り出すごとに最大系列長でパディング\"\"\"\n",
    "    def __init__(self, padding_idx):\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)  # batch内の系列が降順になっているとpaddingの効率がいいらしい\n",
    "        sequences = [x[0] for x in sorted_batch]\n",
    "        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=self.padding_idx)  # padding\n",
    "        labels = torch.LongTensor([x[1] for x in sorted_batch])\n",
    "        lens = torch.LongTensor([x[0].shape[0] for x in sorted_batch])\n",
    "        return sequences_padded, labels, lens\n",
    "\n",
    "\n",
    "# text fileの読み込みを行う関数\n",
    "def load_file(path):\n",
    "    with open(path) as fi:\n",
    "        x, y = [],[]\n",
    "        for line in fi:\n",
    "            title, category = line.strip().split('\\t')\n",
    "            x.append(nltk.word_tokenize(title))\n",
    "            y.append(category)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# 単語のリストからid番号列に変換する関数\n",
    "def words2id(words, word2id):\n",
    "    return torch.tensor([word2id[word] if word in word2id.keys() else 0 for word in words])\n",
    "\n",
    "\n",
    "# カテゴリ名からカテゴリidに変換する関数\n",
    "def cat2id(y):\n",
    "    cate = {'b':0, 't':1, 'e':2, 'm':3}\n",
    "    return torch.tensor(cate[y], dtype=torch.int64)\n",
    "\n",
    "\n",
    "# vocab fileの読み込みを行う関数\n",
    "def load_vocab(vocab_path):\n",
    "    word2id = defaultdict(int)\n",
    "    with open(vocab_path) as vocab_file:\n",
    "        for line in vocab_file:\n",
    "            word, ids = line.strip().split('\\t')\n",
    "            word2id[word] = int(ids)\n",
    "    return word2id\n",
    "\n",
    "\n",
    "# vocabのload\n",
    "WORD2ID = load_vocab('/work01/y_kishinami/100knock-2020/chapter09/work/vocab.txt')\n",
    "logger.info('vocabulary loaded. vocab size: {}'.format(len(WORD2ID) + 1))\n",
    "\n",
    "# datasetのload\n",
    "logger.info('dataset loading ...')\n",
    "X_train, Y_train = load_file('/work01/y_kishinami/100knock-2020/chapter09/work/train.txt')\n",
    "X_valid, Y_valid = load_file('/work01/y_kishinami/100knock-2020/chapter09/work/valid.txt')\n",
    "Train_dataset = CreateDataset(X_train, Y_train, WORD2ID)\n",
    "Valid_dataset = CreateDataset(X_valid, Y_valid, WORD2ID)\n",
    "logger.info('dataset loaded.')\n",
    "\n",
    "\n",
    "# loss, accを計算する関数\n",
    "def calc_loss_and_acc(model, dataset, criterion):\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    total_loss, total, correct = 0., 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs, [inputs[0].shape[0]])\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "            pred = torch.argmax(outputs, dim=-1)\n",
    "            total += len(inputs)\n",
    "            correct += (pred == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "\n",
    "# 学習\n",
    "def train(model, train_dataset, valid_dataset, batch_size, criterion, optimizer, epoch, device, collate_fn=None):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        for data in train_dataloader:\n",
    "            inputs, labels, inputs_len = data\n",
    "            optimizer.zero_grad()\n",
    "            inputs.to(device)\n",
    "            labels.to(device)\n",
    "            inputs_len.to(device)\n",
    "            outputs = model(inputs, inputs_len)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss, train_acc = calc_loss_and_acc(model, train_dataset, criterion)\n",
    "        valid_loss, valid_acc = calc_loss_and_acc(model, valid_dataset, criterion)\n",
    "\n",
    "        print('epoch: {} done. '.format(epoch + 1))\n",
    "        print('train loss: {}\\ttrain acc: {}'.format(train_loss, train_acc))\n",
    "        print('valid loss: {}\\tvalid acc: {}'.format(valid_loss, valid_acc))\n",
    "\n",
    "        best_loss = min(best_loss, valid_loss)\n",
    "        if valid_loss == best_loss:\n",
    "            print('best loss: {}'.format(best_loss))\n",
    "\n",
    "    return best_loss  # validのlossの最小値を返す\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # 1回の組み合わせについての時間指定も可能\n",
    "    # 探索パラメータ\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 5e-4, 5e-2)\n",
    "    batch_size = int(trial.suggest_discrete_uniform('batch_size', 16, 128, 16))\n",
    "    num_layers = int(trial.suggest_int('num_layers', 1, 5))\n",
    "    drop_out = trial.suggest_uniform('drop_out', 0.0, 1.0)\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [50, 128, 256])\n",
    "\n",
    "    # 固定パラメータ\n",
    "    EMB_SIZE = 300\n",
    "    VOCAB_SIZE = len(WORD2ID) + 1\n",
    "    PADDING_IDX = VOCAB_SIZE - 1\n",
    "    OUTPUT_SIZE = 4\n",
    "    NUM_EPOCHS = 5\n",
    "\n",
    "    model = biRNN(EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, hidden_size, num_layers, drop_out)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr_decay=0.001, lr=learning_rate)\n",
    "    valid_loss = train(model, Train_dataset, Valid_dataset, batch_size, criterion, optimizer, NUM_EPOCHS, device, collate_fn=Padsequence(VOCAB_SIZE-1))\n",
    "    return valid_loss\n",
    "\n",
    "\n",
    "def main():\n",
    "    study = optuna.create_study()\n",
    "    study.optimize(objective, timeout=3600)  # 1時間できる\n",
    "    # pandasとかで一覧が出せるので，それをみるのは面白いかも\n",
    "\n",
    "    # print result\n",
    "    print('best trial:')\n",
    "    trial = study.best_trial\n",
    "    print(trial)\n",
    "    print(study.best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89. 事前学習済み言語モデルからの転移学習\n",
    "事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greek 10-year yields rise day after five-year sale\n",
      "[3306, 2184, 1011, 2095, 16189, 4125, 2154, 2044, 2274, 1011, 2095, 5096]\n",
      "['greek', '10', '-', 'year', 'yields', 'rise', 'day', 'after', 'five', '-', 'year', 'sale']\n"
     ]
    }
   ],
   "source": [
    "# テキストデータをtitle, categoryそれぞれのリストとして読み込み\n",
    "train_path = '/Users/y_kishinami/Documents/100knock-2020/y-kishinami/chapter06/work/train.txt'\n",
    "valid_path = '/Users/y_kishinami/Documents/100knock-2020/y-kishinami/chapter06/work/valid.txt'\n",
    "\n",
    "def load_file_q89(path):\n",
    "    with open(path) as fi:\n",
    "        x, y = [],[]\n",
    "        for line in fi:\n",
    "            title, category = line.strip().split('\\t')\n",
    "            #x.append(nltk.word_tokenize(title))\n",
    "            x.append(title)\n",
    "            y.append(category)\n",
    "        return x, y\n",
    "\n",
    "x_train, y_train = load_file_q89(train_path)\n",
    "x_valid, y_valid = load_file_q89(valid_path)\n",
    "print(x_train[0])\n",
    "ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x_train[0]))\n",
    "print(ids)\n",
    "print(tokenizer.convert_ids_to_tokens(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, x, y, tokenizer, max_len):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):  # CreateDataset()[index]で返ってくる値を定義\n",
    "        text = self.x[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        return torch.LongTensor(inputs['input_ids']), torch.LongTensor(inputs['attention_mask']), cat2id(self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,   100,   100, 16189,  4125,  2154,  2044,   100,  5096,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n",
      "['[CLS]', '[UNK]', '[UNK]', 'yields', 'rise', 'day', 'after', '[UNK]', 'sale', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# nltkでトークナイズしたものをBertTokenizerに突っ込んだパターン\n",
    "max_len = 24\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset_train = CreateDataset(x_train, y_train, tokenizer, max_len)\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=2)\n",
    "for data in islice(train_dataloader, 1):\n",
    "    print(data[0][0])\n",
    "    print(tokenizer.convert_ids_to_tokens(data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  3306,  2184,  1011,  2095, 16189,  4125,  2154,  2044,  2274,\n",
      "         1011,  2095,  5096,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n",
      "['[CLS]', 'greek', '10', '-', 'year', 'yields', 'rise', 'day', 'after', 'five', '-', 'year', 'sale', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/y_kishinami/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# titileをそのままBertTokenizerに突っ込んだパターン\n",
    "max_len = 24\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset_train = CreateDataset(x_train, y_train, tokenizer, max_len)\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=2)\n",
    "for data in islice(train_dataloader, 1):\n",
    "    print(data[0][0])\n",
    "    print(tokenizer.convert_ids_to_tokens(data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, drop_rate, otuput_size):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.drop = torch.nn.Dropout(drop_rate)\n",
    "        self.fc = torch.nn.Linear(768, otuput_size)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        outputs = self.bert(ids, attention_mask=mask)\n",
    "        logit = self.fc(outputs[\"pooler_output\"])\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3950,  0.0542,  0.5064,  0.1149]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = 0.4\n",
    "output_size = 4\n",
    "bert = BERTClass(dropout, output_size)\n",
    "\n",
    "ids = torch.tensor([[101, 100, 100, 16189, 4125, 2154, 2044, 100, 5096, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "label = torch.tensor(0)\n",
    "\n",
    "bert(ids, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 実際にGPU上での学習に使用したスクリプト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/q89.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/q89.py\n",
    "import argparse\n",
    "from os import path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from logzero import logger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "nltk.download('punkt')\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, x, y, tokenizer, max_len=50):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):  # CreateDataset()[index]で返ってくる値を定義\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            self.x[index],\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "        return torch.LongTensor(inputs['input_ids']), torch.LongTensor(inputs['attention_mask']), self.y[index]\n",
    "\n",
    "# text fileの読み込みを行う関数\n",
    "def load_file(path):\n",
    "    cate = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "    with open(path) as fi:\n",
    "        x, y = [],[]\n",
    "        for line in fi:\n",
    "            title, category = line.strip().split('\\t')\n",
    "            x.append(title)\n",
    "            y.append(cate[category])\n",
    "        return x, torch.Tensor(y).long()\n",
    "\n",
    "# vocab fileの読み込みを行う関数\n",
    "def load_vocab(vocab_path):\n",
    "    word2id = defaultdict(int)\n",
    "    with open(vocab_path) as vocab_file:\n",
    "        for line in vocab_file:\n",
    "            word, ids = line.strip().split('\\t')\n",
    "            word2id[word] = int(ids)\n",
    "    return word2id\n",
    "\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, drop_rate, otuput_size):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.drop = torch.nn.Dropout(p=drop_rate)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.fc = torch.nn.Linear(768, otuput_size)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        outputs = self.bert(ids, attention_mask=mask)\n",
    "        logit = self.fc(self.drop(outputs[\"pooler_output\"]))\n",
    "        return logit\n",
    "\n",
    "\n",
    "# loss, accを計算する関数\n",
    "def calc_loss_and_acc(model, dataset, criterion, batch_size, device):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    total_loss, total, correct = 0., 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs = data[0].to(device)\n",
    "            mask = data[1].to(device)\n",
    "            labels = data[2].to(device)\n",
    "            outputs = model(inputs, mask)\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "            pred = torch.argmax(outputs, dim=-1)\n",
    "            total += len(inputs)\n",
    "            correct += (pred == labels).sum().item()\n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "\n",
    "# 学習\n",
    "def train(model, train_dataset, valid_dataset, batch_size, criterion, optimizer, epoch, device, writer, collate_fn=None):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        for data in train_dataloader:\n",
    "            inputs = data[0]\n",
    "            mask = data[1]\n",
    "            labels = data[2]\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            mask = mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs, mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss, train_acc = calc_loss_and_acc(model, train_dataset, criterion, batch_size, device)\n",
    "        valid_loss, valid_acc = calc_loss_and_acc(model, valid_dataset, criterion, batch_size, device)\n",
    "\n",
    "        writer.add_scalar('train_loss', train_loss, epoch + 1)\n",
    "        writer.add_scalar('train_acc', train_acc, epoch + 1)\n",
    "        writer.add_scalar('valid_loss', valid_loss, epoch + 1)\n",
    "        writer.add_scalar('valid_acc', valid_acc, epoch + 1)\n",
    "        print('epoch: {} done. '.format(epoch + 1))\n",
    "        print('train loss: {}\\ttrain acc: {}'.format(train_loss, train_acc))\n",
    "        print('valid loss: {}\\tvalid acc: {}'.format(valid_loss, valid_acc))\n",
    "\n",
    "\n",
    "# argument\n",
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser(description='hogehoge')\n",
    "    parser.add_argument('--vocab_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/vocab.txt', type=path.abspath, help='Path to vocabulary file')\n",
    "    parser.add_argument('--train_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/train.txt', type=path.abspath, help='Path to train data file')\n",
    "    parser.add_argument('--valid_path', default='/work01/y_kishinami/100knock-2020/chapter09/work/valid.txt', type=path.abspath, help='Path to valid data file')\n",
    "    parser.add_argument('--log_dir', default='/work01/y_kishinami/100knock-2020/chapter09/work/logs/q89', type=path.abspath, help='tensorboard log dir')\n",
    "    parser.add_argument('--output_size', default=4, type=int, help='dimension of output layer')\n",
    "    parser.add_argument('--hidden_size', default=50, type=int, help='dimension of hidden layer')\n",
    "    parser.add_argument('--batch_size', default=1, type=int, help='batch size')\n",
    "    parser.add_argument('--lr', default=0.00001, type=float, help='learning late')\n",
    "    parser.add_argument('--epoch', default=10, type=int, help='the number of epoch')\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main():\n",
    "    # argument\n",
    "    parser = create_parser()\n",
    "    args = parser.parse_args()\n",
    "    logger.info(args)\n",
    "    writer = SummaryWriter(log_dir=args.log_dir)\n",
    "\n",
    "    # 語彙のload（単語→idへの変換辞書）\n",
    "    word2id = load_vocab(args.vocab_path)\n",
    "    vocab_size = len(word2id) + 1\n",
    "    logger.info('vocabulary loaded. vocab size: {}'.format(vocab_size))\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # datasetのload\n",
    "    logger.info('dataset loading ...')\n",
    "    x_train, y_train = load_file(args.train_path)\n",
    "    x_valid, y_valid = load_file(args.valid_path)\n",
    "    train_dataset = CreateDataset(x_train, y_train, tokenizer)\n",
    "    valid_dataset = CreateDataset(x_valid, y_valid, tokenizer)\n",
    "    logger.info('dataset loaded. ')\n",
    "    logger.info('train data: {} samples'.format(len(train_dataset)))\n",
    "    logger.info('valid data: {} samples'.format(len(valid_dataset)))\n",
    "\n",
    "\n",
    "    # モデルの初期化\n",
    "    bert = BERTClass(0.4, args.output_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer = torch.optim.SGD(bert.parameters(), lr=0.001, momentum=0.9)\n",
    "    optimizer = torch.optim.AdamW(bert.parameters(), lr=0.00001)\n",
    "\n",
    "    # 学習\n",
    "    logger.info('training start !')\n",
    "    train(bert, train_dataset, valid_dataset, args.batch_size, criterion, optimizer, args.epoch, device, writer)\n",
    "    logger.info('training done !')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
